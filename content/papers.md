+++
title = "PAPERS"
+++

<br/>

I usually publish at ML conferences. Sometimes I write off-expertise [articles](https://www.dailysignal.com/2013/06/25/greece-austerity-doesnt-involve-public-sector-layoffs/), too.



------
[**Spread spurious attribute: Improving worst-group accuracy with spurious attribute estimation**](https://openreview.net/forum?id=_F9xpOrqyX9)  
Junhyun Nam, Jaehyung Kim, **JL**, and Jinwoo Shin  
ICLR 2022  


[**Meta-learning sparse implicit neural representations**](https://openreview.net/forum?id=Tn0PnRY877g)  
{**JL**, Jihoon Tack}<sub><small>eq</small></sub>, Namhoon Lee, and Jinwoo Shin  
NeurIPS 2021 (also at [SNN 2021](https://sites.google.com/view/sparsity-workshop-2021/))  

**GreedyPrune: Layer-wise optimization algorithms for magnitude-based pruning**  
Vinoth Nandakumar, and **JL**  
[SNN 2021](https://sites.google.com/view/sparsity-workshop-2021/)  

[**Co<sup><small>2</small></sup>L: Contrastive continual learning**](https://openaccess.thecvf.com/content/ICCV2021/html/Cha_Co2L_Contrastive_Continual_Learning_ICCV_2021_paper.html)  
Hyuntak Cha, **JL**, and Jinwoo Shin  
ICCV 2021  

[**Provable memorization via deep neural networks using sub-linear parameters**](http://proceedings.mlr.press/v134/park21a.html)  
Sejun Park, **JL**, Chulhee Yun, and Jinwoo Shin  
COLT 2021 (also at [Deepmath 2020](https://deepmath-conference.com))  

[**Layer-adaptive sparsity for the magnitude-based pruning**](https://openreview.net/forum?id=H6ATjJ0TKdf)  
**JL**, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin  
ICLR 2021  

[**Minimum width for universal approximation**](https://openreview.net/forum?id=O-XJwyoIF-k)  
Sejun Park, Chulhee Yun, **JL**, and Jinwoo Shin  
ICLR 2021 `spotlight` (also at [Deepmath 2020](https://deepmath-conference.com))  

[**MASKER: Masked keyword regularization for reliable text generation**](https://ojs.aaai.org/index.php/AAAI/article/view/17601)  
{Seung Jun Moon, Sangwoo Mo}<sub>eq</sub>, Kimin Lee, **JL**, and Jinwoo Shin  
AAAI 2021  

[**Learning bounds for risk-sensitive learning**](https://proceedings.neurips.cc/paper/2020/hash/9f60ab2b55468f104055b16df8f69e81-Abstract.html)  
**JL**, Sejun Park, and Jinwoo Shin  
NeurIPS 2020  

[**Learning from failure: Training debiased classifier from biased classifier**](https://proceedings.neurips.cc/paper/2020/hash/eddc3427c5d77843c2253f1e799fe933-Abstract.html)  
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, **JL**, and Jinwoo Shin  
NeurIPS 2020  

[**Lookahead: A far-sighted alternative of magnitude-based pruning**](https://openreview.net/forum?id=ryl3ygHYDB)  
{Sejun Park, **JL**}<sub>eq</sub>, Sangwoo Mo, and Jinwoo Shin  
ICLR 2020  

[**Learning finite-dimensional coding schemes with nonlinear reconstruction maps**](https://epubs.siam.org/doi/abs/10.1137/18M1234461)  
**JL** and Maxim Raginsky  
SIMODS 2019  

[**Minimax statistical learning with Wasserstein distances**](https://proceedings.neurips.cc/paper/2018/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html)  
**JL** and Maxim Raginsky  
NeurIPS 2018 `spotlight`  

[**On MMSE estimation from quantized observations in the nonasymptotic regime**](https://ieeexplore.ieee.org/document/7282992)  
**JL**, Maxim Raginsky, and Pierre Moulin  
ISIT 2015  