+++
title = "RESEARCH"
+++
<br/>

Simply put, I am trying to figure out how to survive the age of *large-scale ML models*---neural nets are growing rapidly in size, making the training / inference / other costs unaffordable. This trend opens up many research questions that are left for us to answer, including:
- **Compression:** How can we make an already-trained model smaller, so that everybody (with a tiny hardware) can use it?
- **TinyML:** How can we effectively train a neural net on a tiny device? If we can communicate with a large-scale model, how much can it help?
- **Efficient Models:** Can we make a new model, that is both expressive enough to learn the target function, and light enough to be trained with a small training budget?

I hope to use every tool at hand---algorithm, theory, system-level tricks---that is required to address this challenge.